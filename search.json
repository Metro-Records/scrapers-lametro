[
  {
    "objectID": "local-development.html",
    "href": "local-development.html",
    "title": "Local development",
    "section": "",
    "text": "The scrapers are bundled with a docker-compose.yml file that will allow you to run them on your machine.\nTo scrape all recently updated data, run:\ndocker compose run --rm scrapers\nTo run a particular scrape or pass arguments to pupa, append your command to the end of the previous command, like:\n# Scrape board reports from the last week\ndocker compose run --rm scrapers pupa update lametro bills window=7\n\n\nIf you’d like to scrape data into a Councilmatic instance for easy viewing, first run your local instance of LA Metro Councilmatic as normal.\nThen, in your local scraper repository, run your scrapes using the docker-compose.councilmatic.yml file:\ndocker compose -f docker-compose.councilmatic.yml run --rm scrapers\n\n\n\n\n\nusage: pupa update [-h] [--scrape] [--import] [--nonstrict] [--fastmode] [--datadir SCRAPED_DATA_DIR] [--cachedir CACHE_DIR]\n                   [-r SCRAPELIB_RPM] [--timeout SCRAPELIB_TIMEOUT] [--no-verify] [--retries SCRAPELIB_RETRIES]\n                   [--retry_wait SCRAPELIB_RETRY_WAIT_SECONDS]\n                   module\n\nupdate pupa data\n\npositional arguments:\n  module                path to scraper module\n\noptions:\n  -h, --help            show this help message and exit\n  --scrape              only run scrape post-scrape step\n  --import              only run import post-scrape step\n  --nonstrict           skip validation on save\n  --fastmode            use cache and turn off throttling\n  --datadir SCRAPED_DATA_DIR\n                        data directory\n  --cachedir CACHE_DIR  cache directory\n  -r SCRAPELIB_RPM, --rpm SCRAPELIB_RPM\n                        scraper rpm\n  --timeout SCRAPELIB_TIMEOUT\n                        scraper timeout\n  --no-verify           skip tls verification\n  --retries SCRAPELIB_RETRIES\n                        scraper retries\n  --retry_wait SCRAPELIB_RETRY_WAIT_SECONDS\n                        scraper retry wait\n\n\n\n\n\n\nTip\n\n\n\nRunning a scrape with --fastmode will disable request throttling, resulting in a faster scrape. Great for local development, especially for narrow scrapes, e.g.,\npupa update --fastmode lametro events window=1\n\n\n\n\n\nbills\n\nwindow (default: 28) - How far back to scrape, in days. Scrapes all matters, if 0.\nmatter_ids (default: None) - Comma-separated list of MatterIds from the Legistar API. Scrapes all matters updated within window, if None.\n\nevents\n\nwindow (default: None) - How far back to scrape, in days.\n\n\n\n\n\n# Scrape board reports from the past week\npupa update lametro bills window=7\n\n# Scrape specific board reports\npupa update lametro bills matter_ids=10340,10084\n\n# Scrape events from past 30 days\npupa update lametro events window=30\n\n\n\n\nusage: pupa clean [-h] [--window WINDOW] [--max MAX] [--report] [--yes]\n\nRemoves database objects that haven't been seen in recent scrapes\n\noptions:\n  -h, --help       show this help message and exit\n  --window WINDOW  objects not seen in this many days will be deleted from the database\n  --max MAX        max number of objects to delete without triggering failsafe\n  --report         generate a report of what objects this command would delete without making any changes to the database\n  --yes            assumes an answer of 'yes' to all interactive prompts\n\n\n# Log which objects will be deleted without making changes to the database\npupa clean --report\n\n# Remove objects that haven't been seen for 30 days\npupa clean --window 30\n\n# Remove a maximum of 100 objects\npupa clean --max 100",
    "crumbs": [
      "Local development"
    ]
  },
  {
    "objectID": "local-development.html#running-the-scrapers",
    "href": "local-development.html#running-the-scrapers",
    "title": "Local development",
    "section": "",
    "text": "The scrapers are bundled with a docker-compose.yml file that will allow you to run them on your machine.\nTo scrape all recently updated data, run:\ndocker compose run --rm scrapers\nTo run a particular scrape or pass arguments to pupa, append your command to the end of the previous command, like:\n# Scrape board reports from the last week\ndocker compose run --rm scrapers pupa update lametro bills window=7\n\n\nIf you’d like to scrape data into a Councilmatic instance for easy viewing, first run your local instance of LA Metro Councilmatic as normal.\nThen, in your local scraper repository, run your scrapes using the docker-compose.councilmatic.yml file:\ndocker compose -f docker-compose.councilmatic.yml run --rm scrapers\n\n\n\n\n\nusage: pupa update [-h] [--scrape] [--import] [--nonstrict] [--fastmode] [--datadir SCRAPED_DATA_DIR] [--cachedir CACHE_DIR]\n                   [-r SCRAPELIB_RPM] [--timeout SCRAPELIB_TIMEOUT] [--no-verify] [--retries SCRAPELIB_RETRIES]\n                   [--retry_wait SCRAPELIB_RETRY_WAIT_SECONDS]\n                   module\n\nupdate pupa data\n\npositional arguments:\n  module                path to scraper module\n\noptions:\n  -h, --help            show this help message and exit\n  --scrape              only run scrape post-scrape step\n  --import              only run import post-scrape step\n  --nonstrict           skip validation on save\n  --fastmode            use cache and turn off throttling\n  --datadir SCRAPED_DATA_DIR\n                        data directory\n  --cachedir CACHE_DIR  cache directory\n  -r SCRAPELIB_RPM, --rpm SCRAPELIB_RPM\n                        scraper rpm\n  --timeout SCRAPELIB_TIMEOUT\n                        scraper timeout\n  --no-verify           skip tls verification\n  --retries SCRAPELIB_RETRIES\n                        scraper retries\n  --retry_wait SCRAPELIB_RETRY_WAIT_SECONDS\n                        scraper retry wait\n\n\n\n\n\n\nTip\n\n\n\nRunning a scrape with --fastmode will disable request throttling, resulting in a faster scrape. Great for local development, especially for narrow scrapes, e.g.,\npupa update --fastmode lametro events window=1\n\n\n\n\n\nbills\n\nwindow (default: 28) - How far back to scrape, in days. Scrapes all matters, if 0.\nmatter_ids (default: None) - Comma-separated list of MatterIds from the Legistar API. Scrapes all matters updated within window, if None.\n\nevents\n\nwindow (default: None) - How far back to scrape, in days.\n\n\n\n\n\n# Scrape board reports from the past week\npupa update lametro bills window=7\n\n# Scrape specific board reports\npupa update lametro bills matter_ids=10340,10084\n\n# Scrape events from past 30 days\npupa update lametro events window=30\n\n\n\n\nusage: pupa clean [-h] [--window WINDOW] [--max MAX] [--report] [--yes]\n\nRemoves database objects that haven't been seen in recent scrapes\n\noptions:\n  -h, --help       show this help message and exit\n  --window WINDOW  objects not seen in this many days will be deleted from the database\n  --max MAX        max number of objects to delete without triggering failsafe\n  --report         generate a report of what objects this command would delete without making any changes to the database\n  --yes            assumes an answer of 'yes' to all interactive prompts\n\n\n# Log which objects will be deleted without making changes to the database\npupa clean --report\n\n# Remove objects that haven't been seen for 30 days\npupa clean --window 30\n\n# Remove a maximum of 100 objects\npupa clean --max 100",
    "crumbs": [
      "Local development"
    ]
  },
  {
    "objectID": "local-development.html#writing-tests",
    "href": "local-development.html#writing-tests",
    "title": "Local development",
    "section": "Writing tests",
    "text": "Writing tests\ntktktk",
    "crumbs": [
      "Local development"
    ]
  },
  {
    "objectID": "scrapers/people.html",
    "href": "scrapers/people.html",
    "title": "Board Member and Legislative Body Scrapers",
    "section": "",
    "text": "Located in lametro/people.py\nCreates the board and its committees (or, bodies of type “Committee” or “Independent Taxpayer Oversight Committee”)\nCreates members of the above bodies and membership objects for each of their terms\nFurther reading: https://open-civic-data.readthedocs.io/en/latest/proposals/0005.html",
    "crumbs": [
      "Scrapers",
      "Board Member and Legislative Body Scrapers"
    ]
  },
  {
    "objectID": "scrapers/people.html#key-issues",
    "href": "scrapers/people.html#key-issues",
    "title": "Board Member and Legislative Body Scrapers",
    "section": "Key issues",
    "text": "Key issues\n\nMemberships are tricky! Sometimes, they are updated in ways that don’t resolve correctly, introducing duplicates. Other times, they are deleted from Legistar, which is a challenge for us to detect.\n\nConsider start date in addition to, or instead of, end date when importing memberships\nCommittees: Old Committee members showing\nFind a mechanism to handle data deleted from the source system",
    "crumbs": [
      "Scrapers",
      "Board Member and Legislative Body Scrapers"
    ]
  },
  {
    "objectID": "scrapers/events.html",
    "href": "scrapers/events.html",
    "title": "Event Scraper",
    "section": "",
    "text": "Located in lametro/events.py\nCreates events",
    "crumbs": [
      "Scrapers",
      "Event Scraper"
    ]
  },
  {
    "objectID": "scrapers/events.html#key-issues",
    "href": "scrapers/events.html#key-issues",
    "title": "Event Scraper",
    "section": "Key issues",
    "text": "Key issues\n\nMetro streams audio in both English and Spanish. They cannot associate multiple broadcast link with one event in Legistar, so they create two nearly identical events that we merge during scrapes.\n\nImport and display the Spanish language audio\n“Ver en español” link not visible\n\nThe board approves minutes for their previous meeting each time they meet. Sometimes, these minutes are explicitly associated with the event. When they aren’t, we try to guess the approved minutes file.\n\nFind approved minutes\nUpgrades to the LA Metro scraper\nEvent scrape: If there is more than one minutes file, which one should we use?\nAccount for multiple meeting minutes",
    "crumbs": [
      "Scrapers",
      "Event Scraper"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LA Metro Scrapers Documentation",
    "section": "",
    "text": "Welcome to the documentation for the LA Metro Scrapers! Here, you’ll find information about local development, deployment, and an overview of each scraper (and decisions that we’ve made about them)."
  },
  {
    "objectID": "index.html#how-do-they-work",
    "href": "index.html#how-do-they-work",
    "title": "LA Metro Scrapers Documentation",
    "section": "How do they work?",
    "text": "How do they work?\nAt a high level, the scrapers retrieve information from Metro instances of the Legistar interface, also known as InSite, and the Legistar API (endpoints at https://webapi.legistar.com/metro/*).\nSee the relevant scraper documentation for more information about where information comes from, and how it is parsed."
  },
  {
    "objectID": "index.html#how-are-they-run",
    "href": "index.html#how-are-they-run",
    "title": "LA Metro Scrapers Documentation",
    "section": "How are they run?",
    "text": "How are they run?\nThe scrapers are run by Airflow and populate LA Metro Councilmatic instances, outlined below.\n\n\n\nScraper image tag\nAirflow instance\nMetro instance\n\n\n\n\nmain\nhttps://la-metro-dashboard-heroku.datamade.us/home\nhttps://la-metro-councilmatic-staging.herokuapp.com/\n\n\ndeploy\nhttps://la-metro-dashboard-heroku-prod.datamade.us/home\nhttps://boardagendas.metro.net\n\n\n\nSee Deployment for more on how scraper image tags are built."
  },
  {
    "objectID": "index.html#what-do-they-depend-on",
    "href": "index.html#what-do-they-depend-on",
    "title": "LA Metro Scrapers Documentation",
    "section": "What do they depend on?",
    "text": "What do they depend on?\nThe scrapers have a couple of key dependencies.\n\npupa is the framework for scraping and organizing data according to the Open Civic Data standard. Our scrapers are subclasses of pupa.Scraper, and we use the pupa CLI to run scrapes.\n\nSee Useful pupa commands for more on the CLI.\n\npython-legistar-scraper is a Python wrapper for InSite and the Legistar API that we use to retrieve data. Our scrapers are also subclasses of the relevant LegistarScraper subclasses from this library."
  },
  {
    "objectID": "debugging.html",
    "href": "debugging.html",
    "title": "Debugging",
    "section": "",
    "text": "Many issues can arise in the Metro galaxy, from the shallowest part of the frontend to the deepest depths of the backend. However, these issues generally fall into two broad categories:\n\nData is missing\nData is incorrect\nAppendices\n\nMore on Airflow\nView an entity in the Councilmatic database\nInspect the scraper logs\n\n\n\n\n\nThis section expounds on common culprits in our three categories of failure: missing data, incorrect data, and issues with metadata. The culprits are ordered from most to least likely, therefore we suggest moving through the category relevant to your problem in order until you identify the issue.\n\n\n\n\nOur scrapers are brittle by design, i.e., they will generally fail if they try to ingest data that is formatted incorrectly. If there has been a scrape failure, there should be a corresponding exception in the scrapers-lametro project in Sentry.\nCommon exceptions include OperationalError, which indicates that something went awry with the server (e.g., not enough disk space) and ScraperError, which indicates a particular issue with the one of the scrapers.\nIf you don’t see an issue in Sentry, but you have reason to believe there was an error (or that our integration with Sentry is faulty), you can follow our documentation for finding errors in the scraper logs.\nIf there is not an error in Sentry or in the scraper logs, then the scrape ran.\n\n\n\nIf scrapes are running, by far the most common source of issues is the scraper failing to capture changes to a bill or event. The root issue is that we rely on timestamps that should indicate an update to determine which bills and events to scrape. In reality, these timestamps do not always update when a change is made to an event or bill in Legistar. We have a couple of strategies to get around this:\n\nGenerally, Metro staff post agendas the Friday before meetings occur. Thus, on Fridays, we scrape all events and bills at the top of every hour.\nWhen we run windowed scrapes, we scrape all events and bills with timestamps within or after the given window. This is because upcoming events and bills are the ones that are most likely to change. For example, a scrape with a one-hour window will scrape any events that have changed in the past hour, plus any events with a future start date. Here is a complete list of the timestamps we consider when scraping events and bills.\n\nTaken together, these strategies should ensure that any change appears on the Metro site within an hour of being made, however edge cases can happen!\nTo determine whether timestamps are causing the problem, follow our documentation for viewing an entity to ascertain when an entity was last updated in the Councilmatic database and compare it to the timestamps we consider in windowed scrapes. If the entity has not been updated in Councilmatic since the latest timestamp in the Legistar API, trigger a broader scrape.\n\n\n\nIf the scrapes and ETL pipeline are running as expected, but data is missing, then there is a deeper issue. A good first question: What are the most recent changes in the scraper codebase? Could this have caused unusual behavior?\nThe scrapers work through the cooperation of several repos, and the bug fix may require investigating one or more of these repos.\n\nMetro-Records/scrapers-lametro contains Metro-specific code for the Bill, Event, and Person scrapers. If you need to patch the scraper code, create a PR against this repo.\nMetro-Records/la-metro-dashboard is the Airflow app that schedules scrapes and the scripts that define them. If you need to change the scheduling of scrapes, create a PR against this repo.\nopencivicdata/python-legistar-scraper/tree/master/legistar contains the LegistarScraper and LegistarAPIScraper variants, from which the Metro scrapers inherit. If you need to patch the Legistar scraping code, create a PR against this repo.\nAll scrapers depend on the pupa framework for scraping and importing data using the OCD standard. In the unlikely event that you need to patch pupa, create a fork, then submit a PR against this repo.\n\n\n\n\n\n\n\nData issues can occur when the Legistar API or web interface displays the wrong information. This generally happens when Metro staff enters information that is incorrect or is organized differently than our scraper expects.\nIf Metro reports that data is incorrect, follow our documentation for viewing an entity to inspect the problematic object and view its sources in the Legistar API. If the erroneous data matches the API sources, report the error the Metro and wait for them to resolve the issue or clarify how to interpret the data.\n\n\n\npupa, our scraping framework, doesn’t know how to identify information that has been deleted. Sometimes, we scrape information that Metro later removes.\nIf this happens for a bill, event, or membership, shell into the server and remove the erroneous entity through the ORM. N.b., it’s easiest to get at this through the relevant person, e.g.,\nfrom lametro.models import *\nLAMetroPerson.objects.get(family_name='Mitchell')\n&lt;LAMetroPerson: Holly Mitchell&gt;\nLAMetroPerson.objects.get(family_name='Mitchell').memberships.filter(organization__name__endswith='Committee')\n&lt;QuerySet [&lt;Membership: Holly Mitchell in Operations, Safety, and Customer Experience Committee (Member)&gt;, &lt;Membership: Holly Mitchell in Finance, Budget and Audit Committee (Member)&gt;]&gt;\nLAMetroPerson.objects.get(family_name='Mitchell').memberships.filter(organization__name__endswith='Committee').count()\n2\nLAMetroPerson.objects.get(family_name='Mitchell').memberships.filter(organization__name__endswith='Committee').delete()\n(2, {'lametro.Membership': 2})\n\n\n\nSometimes, there is not a problem with the data, but rather there is an error in how it is displayed in the Metro Councilmatic instance. This class of issues is generally very rare. As with deeper issues with the scrapers, a helpful first question is: What are the most recent changes, and could they be the source of the problem you’re seeing?\nBase models and view logic are defined in django-councilmatic. Metro generally uses the most recent release of the 2.5.x series, so be sure you’re consulting the 2.5 branch of django-councilmatic when you start spelunking.\nMetro makes a number of customizations to the models and view logic in this repository. Consult the most recent release to view the code that’s deployed to production.\n\n\n\n\n\n\n\nThe Metro data pipeline, both scrapes and management commands to perform subsequent ETL, are scheduled and run by our Metro Airflow instance, located at https://la-metro-dashboard-heroku-prod.datamade.us. Consult the DataMade BitWarden for login credentials. (Search metro-support@datamade.us in our shared folder!)\nThe dashboard lives on GitHub, here.\nApache maintains thorough documentation of core concepts, as well as navigating the UI. If you’ve never used Airflow before, these are great resources to start with!\n\n\n\nIf there is an issue with a particular entity, view that entity’s detail page on the board agendas site (https://boardagendas.metro.net). Links to source data from Legistar will be logged to your browser’s developer console.\n\n\n\n\n\nShell into a running instance of LA Metro Councilmatic using either the Heroku CLI:\nheroku login\nheroku ps:exec --app=lametro-councilmatic-production\nor Heroku’s web-based console:\n\nwith python manage.py shell.\nRetrieve the problematic entity using its slug, which you can retrieve from the URL of its detail page.\n# In the Django shell\n&gt;&gt;&gt; from lametro.models import *\n&gt;&gt;&gt; entity = LAMetroEvent.objects.get(slug='regular-board-meeting-036b08c9a3f3')\nYou can use the same ORM query to retrieve any entity. Simply swap out LAMetroEvent for the correct model and, of course, update the OCD ID.\n\n\n\nEntity\nModel\n\n\n\n\nPerson\nLAMetroPerson\n\n\nCommittee\nLAMetroOrganization\n\n\nBill\nLAMetroBill\n\n\nEvent\nLAMetroEvent\n\n\n\n\n\n\nAssuming you have retrieved the entity as illustrated in the previous step, you can view its last updated date like this:\n&gt;&gt;&gt; entity.updated_at\ndatetime.datetime(2020, 3, 25, 0, 47, 3, 471572, tzinfo=&lt;UTC&gt;)\nYou can also view its sources like this:\n&gt;&gt;&gt; import pprint\n&gt;&gt;&gt; pprint.pprint([(source.note, source.url) for source in entity.sources.all()])\n[('api', 'http://webapi.legistar.com/v1/metro/events/1384'),\n ('api (sap)', 'http://webapi.legistar.com/v1/metro/events/1493'),\n ('web',\n  'https://metro.legistar.com/MeetingDetail.aspx?LEGID=1384&GID=557&G=A5FAA737-A54D-4A6C-B1E8-FF70F765FA94'),\n ('web (sap)',\n  'https://metro.legistar.com/MeetingDetail.aspx?LEGID=1493&GID=557&G=A5FAA737-A54D-4A6C-B1E8-FF70F765FA94')]\nEvents have Spanish language sources (e.g., “api (sap)”), as well. In initial debugging, focus on the “api” and “web” sources – by visiting these links and checking that the data in Legistar appears as expected.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe timestamps in the scraper logs are in Chicago local time!\n\n\nIf you have reason to believe the scrape has failed, but you don’t see an exception in Sentry, you can make double sure by consulting the logs for the scraping DAGs in the dashboard.\n\n\nTo confirm the scrape ran, click the name of a DAG to pull up the tree view, which will display the status of the last 25 DAG runs.\n\n\n\nwindowed_bill_scraping DAG tree view\n\n\nNote that the scraping DAGs employ branch operators to determine what kind of scraping task to run. Be sure to look closely to verify that the task you’re expecting is green (succeeded), not pink (skipped).\n\n\n\nThe dashboard has DAGs corresponding to the full overnight scrape, windowed scrapes, fast full scrapes, and hourly processing. To view the logs associated with tasks in a particular DAG, click the name of the DAG to go to the Tree View, which shows you the last 24 runs of the DAG. Then, click the box associated with the task for which you’d like the view the logs. This will pop open a window containing, among other things, a link to the logs. Click the link to view the logs!\n\n\n\nView the logs for the most recent convert_attachment_text run\n\n\nIf there has been an error, the logs for the implicated task should contain something like this:\nimport memberships...\nTraceback (most recent call last):\n  File \"/home/datamade/.virtualenvs/opencivicdata/bin/pupa\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/pupa/cli/__main__.py\", line 68, in main\n    subcommands[args.subcommand].handle(args, other)\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/pupa/cli/commands/update.py\", line 278, in handle\n    return self.do_handle(args, other, juris)\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/pupa/cli/commands/update.py\", line 329, in do_handle\n    report['import'] = self.do_import(juris, args)\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/pupa/cli/commands/update.py\", line 216, in do_import\n    report.update(membership_importer.import_directory(datadir))\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/pupa/importers/base.py\", line 197, in import_directory\n    return self.import_data(json_stream())\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/pupa/importers/base.py\", line 234, in import_data\n    obj_id, what = self.import_item(data)\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/pupa/importers/base.py\", line 258, in import_item\n    obj = self.get_object(data)\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/pupa/importers/memberships.py\", line 36, in get_object\n    return self.model_class.objects.get(**spec)\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/django/db/models/manager.py\", line 82, in manager_method\n    return getattr(self.get_queryset(), name)(*args, **kwargs)\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/django/db/models/query.py\", line 412, in get\n    (self.model._meta.object_name, num)\nopencivicdata.core.models.people_orgs.MultipleObjectsReturned: get() returned more than one Membership -- it returned 2!\nSentry is attempting to send 1 pending error messages\nWaiting up to 10 seconds\nPress Ctrl-C to quit\n05/02/2020 00:40:09 INFO pupa: save jurisdiction New York City as jurisdiction_ocd-jurisdiction-country:us-state:ny-place:new_york-government.json\n05/02/2020 00:40:09 INFO pupa: save organization New York City Council as organization_62bd3c8a-8c37-11ea-b678-122a3d729da3.json\n05/02/2020 00:40:09 INFO pupa: save post District 1 as post_62bdb9c6-8c37-11ea-b678-122a3d729da3.json\nThe timestamps at the bottom correspond to the scrape after the error occurred, so you can use them to determine when the broken scrape occurred and whether it’s relevant to the missing data.",
    "crumbs": [
      "Debugging"
    ]
  },
  {
    "objectID": "debugging.html#categories-of-failure",
    "href": "debugging.html#categories-of-failure",
    "title": "Debugging",
    "section": "",
    "text": "This section expounds on common culprits in our three categories of failure: missing data, incorrect data, and issues with metadata. The culprits are ordered from most to least likely, therefore we suggest moving through the category relevant to your problem in order until you identify the issue.\n\n\n\n\nOur scrapers are brittle by design, i.e., they will generally fail if they try to ingest data that is formatted incorrectly. If there has been a scrape failure, there should be a corresponding exception in the scrapers-lametro project in Sentry.\nCommon exceptions include OperationalError, which indicates that something went awry with the server (e.g., not enough disk space) and ScraperError, which indicates a particular issue with the one of the scrapers.\nIf you don’t see an issue in Sentry, but you have reason to believe there was an error (or that our integration with Sentry is faulty), you can follow our documentation for finding errors in the scraper logs.\nIf there is not an error in Sentry or in the scraper logs, then the scrape ran.\n\n\n\nIf scrapes are running, by far the most common source of issues is the scraper failing to capture changes to a bill or event. The root issue is that we rely on timestamps that should indicate an update to determine which bills and events to scrape. In reality, these timestamps do not always update when a change is made to an event or bill in Legistar. We have a couple of strategies to get around this:\n\nGenerally, Metro staff post agendas the Friday before meetings occur. Thus, on Fridays, we scrape all events and bills at the top of every hour.\nWhen we run windowed scrapes, we scrape all events and bills with timestamps within or after the given window. This is because upcoming events and bills are the ones that are most likely to change. For example, a scrape with a one-hour window will scrape any events that have changed in the past hour, plus any events with a future start date. Here is a complete list of the timestamps we consider when scraping events and bills.\n\nTaken together, these strategies should ensure that any change appears on the Metro site within an hour of being made, however edge cases can happen!\nTo determine whether timestamps are causing the problem, follow our documentation for viewing an entity to ascertain when an entity was last updated in the Councilmatic database and compare it to the timestamps we consider in windowed scrapes. If the entity has not been updated in Councilmatic since the latest timestamp in the Legistar API, trigger a broader scrape.\n\n\n\nIf the scrapes and ETL pipeline are running as expected, but data is missing, then there is a deeper issue. A good first question: What are the most recent changes in the scraper codebase? Could this have caused unusual behavior?\nThe scrapers work through the cooperation of several repos, and the bug fix may require investigating one or more of these repos.\n\nMetro-Records/scrapers-lametro contains Metro-specific code for the Bill, Event, and Person scrapers. If you need to patch the scraper code, create a PR against this repo.\nMetro-Records/la-metro-dashboard is the Airflow app that schedules scrapes and the scripts that define them. If you need to change the scheduling of scrapes, create a PR against this repo.\nopencivicdata/python-legistar-scraper/tree/master/legistar contains the LegistarScraper and LegistarAPIScraper variants, from which the Metro scrapers inherit. If you need to patch the Legistar scraping code, create a PR against this repo.\nAll scrapers depend on the pupa framework for scraping and importing data using the OCD standard. In the unlikely event that you need to patch pupa, create a fork, then submit a PR against this repo.\n\n\n\n\n\n\n\nData issues can occur when the Legistar API or web interface displays the wrong information. This generally happens when Metro staff enters information that is incorrect or is organized differently than our scraper expects.\nIf Metro reports that data is incorrect, follow our documentation for viewing an entity to inspect the problematic object and view its sources in the Legistar API. If the erroneous data matches the API sources, report the error the Metro and wait for them to resolve the issue or clarify how to interpret the data.\n\n\n\npupa, our scraping framework, doesn’t know how to identify information that has been deleted. Sometimes, we scrape information that Metro later removes.\nIf this happens for a bill, event, or membership, shell into the server and remove the erroneous entity through the ORM. N.b., it’s easiest to get at this through the relevant person, e.g.,\nfrom lametro.models import *\nLAMetroPerson.objects.get(family_name='Mitchell')\n&lt;LAMetroPerson: Holly Mitchell&gt;\nLAMetroPerson.objects.get(family_name='Mitchell').memberships.filter(organization__name__endswith='Committee')\n&lt;QuerySet [&lt;Membership: Holly Mitchell in Operations, Safety, and Customer Experience Committee (Member)&gt;, &lt;Membership: Holly Mitchell in Finance, Budget and Audit Committee (Member)&gt;]&gt;\nLAMetroPerson.objects.get(family_name='Mitchell').memberships.filter(organization__name__endswith='Committee').count()\n2\nLAMetroPerson.objects.get(family_name='Mitchell').memberships.filter(organization__name__endswith='Committee').delete()\n(2, {'lametro.Membership': 2})\n\n\n\nSometimes, there is not a problem with the data, but rather there is an error in how it is displayed in the Metro Councilmatic instance. This class of issues is generally very rare. As with deeper issues with the scrapers, a helpful first question is: What are the most recent changes, and could they be the source of the problem you’re seeing?\nBase models and view logic are defined in django-councilmatic. Metro generally uses the most recent release of the 2.5.x series, so be sure you’re consulting the 2.5 branch of django-councilmatic when you start spelunking.\nMetro makes a number of customizations to the models and view logic in this repository. Consult the most recent release to view the code that’s deployed to production.",
    "crumbs": [
      "Debugging"
    ]
  },
  {
    "objectID": "debugging.html#inspecting-data-and-logs",
    "href": "debugging.html#inspecting-data-and-logs",
    "title": "Debugging",
    "section": "",
    "text": "The Metro data pipeline, both scrapes and management commands to perform subsequent ETL, are scheduled and run by our Metro Airflow instance, located at https://la-metro-dashboard-heroku-prod.datamade.us. Consult the DataMade BitWarden for login credentials. (Search metro-support@datamade.us in our shared folder!)\nThe dashboard lives on GitHub, here.\nApache maintains thorough documentation of core concepts, as well as navigating the UI. If you’ve never used Airflow before, these are great resources to start with!\n\n\n\nIf there is an issue with a particular entity, view that entity’s detail page on the board agendas site (https://boardagendas.metro.net). Links to source data from Legistar will be logged to your browser’s developer console.\n\n\n\n\n\nShell into a running instance of LA Metro Councilmatic using either the Heroku CLI:\nheroku login\nheroku ps:exec --app=lametro-councilmatic-production\nor Heroku’s web-based console:\n\nwith python manage.py shell.\nRetrieve the problematic entity using its slug, which you can retrieve from the URL of its detail page.\n# In the Django shell\n&gt;&gt;&gt; from lametro.models import *\n&gt;&gt;&gt; entity = LAMetroEvent.objects.get(slug='regular-board-meeting-036b08c9a3f3')\nYou can use the same ORM query to retrieve any entity. Simply swap out LAMetroEvent for the correct model and, of course, update the OCD ID.\n\n\n\nEntity\nModel\n\n\n\n\nPerson\nLAMetroPerson\n\n\nCommittee\nLAMetroOrganization\n\n\nBill\nLAMetroBill\n\n\nEvent\nLAMetroEvent\n\n\n\n\n\n\nAssuming you have retrieved the entity as illustrated in the previous step, you can view its last updated date like this:\n&gt;&gt;&gt; entity.updated_at\ndatetime.datetime(2020, 3, 25, 0, 47, 3, 471572, tzinfo=&lt;UTC&gt;)\nYou can also view its sources like this:\n&gt;&gt;&gt; import pprint\n&gt;&gt;&gt; pprint.pprint([(source.note, source.url) for source in entity.sources.all()])\n[('api', 'http://webapi.legistar.com/v1/metro/events/1384'),\n ('api (sap)', 'http://webapi.legistar.com/v1/metro/events/1493'),\n ('web',\n  'https://metro.legistar.com/MeetingDetail.aspx?LEGID=1384&GID=557&G=A5FAA737-A54D-4A6C-B1E8-FF70F765FA94'),\n ('web (sap)',\n  'https://metro.legistar.com/MeetingDetail.aspx?LEGID=1493&GID=557&G=A5FAA737-A54D-4A6C-B1E8-FF70F765FA94')]\nEvents have Spanish language sources (e.g., “api (sap)”), as well. In initial debugging, focus on the “api” and “web” sources – by visiting these links and checking that the data in Legistar appears as expected.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe timestamps in the scraper logs are in Chicago local time!\n\n\nIf you have reason to believe the scrape has failed, but you don’t see an exception in Sentry, you can make double sure by consulting the logs for the scraping DAGs in the dashboard.\n\n\nTo confirm the scrape ran, click the name of a DAG to pull up the tree view, which will display the status of the last 25 DAG runs.\n\n\n\nwindowed_bill_scraping DAG tree view\n\n\nNote that the scraping DAGs employ branch operators to determine what kind of scraping task to run. Be sure to look closely to verify that the task you’re expecting is green (succeeded), not pink (skipped).\n\n\n\nThe dashboard has DAGs corresponding to the full overnight scrape, windowed scrapes, fast full scrapes, and hourly processing. To view the logs associated with tasks in a particular DAG, click the name of the DAG to go to the Tree View, which shows you the last 24 runs of the DAG. Then, click the box associated with the task for which you’d like the view the logs. This will pop open a window containing, among other things, a link to the logs. Click the link to view the logs!\n\n\n\nView the logs for the most recent convert_attachment_text run\n\n\nIf there has been an error, the logs for the implicated task should contain something like this:\nimport memberships...\nTraceback (most recent call last):\n  File \"/home/datamade/.virtualenvs/opencivicdata/bin/pupa\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/pupa/cli/__main__.py\", line 68, in main\n    subcommands[args.subcommand].handle(args, other)\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/pupa/cli/commands/update.py\", line 278, in handle\n    return self.do_handle(args, other, juris)\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/pupa/cli/commands/update.py\", line 329, in do_handle\n    report['import'] = self.do_import(juris, args)\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/pupa/cli/commands/update.py\", line 216, in do_import\n    report.update(membership_importer.import_directory(datadir))\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/pupa/importers/base.py\", line 197, in import_directory\n    return self.import_data(json_stream())\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/pupa/importers/base.py\", line 234, in import_data\n    obj_id, what = self.import_item(data)\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/pupa/importers/base.py\", line 258, in import_item\n    obj = self.get_object(data)\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/pupa/importers/memberships.py\", line 36, in get_object\n    return self.model_class.objects.get(**spec)\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/django/db/models/manager.py\", line 82, in manager_method\n    return getattr(self.get_queryset(), name)(*args, **kwargs)\n  File \"/home/datamade/.virtualenvs/opencivicdata/lib/python3.5/site-packages/django/db/models/query.py\", line 412, in get\n    (self.model._meta.object_name, num)\nopencivicdata.core.models.people_orgs.MultipleObjectsReturned: get() returned more than one Membership -- it returned 2!\nSentry is attempting to send 1 pending error messages\nWaiting up to 10 seconds\nPress Ctrl-C to quit\n05/02/2020 00:40:09 INFO pupa: save jurisdiction New York City as jurisdiction_ocd-jurisdiction-country:us-state:ny-place:new_york-government.json\n05/02/2020 00:40:09 INFO pupa: save organization New York City Council as organization_62bd3c8a-8c37-11ea-b678-122a3d729da3.json\n05/02/2020 00:40:09 INFO pupa: save post District 1 as post_62bdb9c6-8c37-11ea-b678-122a3d729da3.json\nThe timestamps at the bottom correspond to the scrape after the error occurred, so you can use them to determine when the broken scrape occurred and whether it’s relevant to the missing data.",
    "crumbs": [
      "Debugging"
    ]
  },
  {
    "objectID": "deployment.html",
    "href": "deployment.html",
    "title": "Deployment",
    "section": "",
    "text": "We deploy the scrapers as tagged Docker images. Tagged images are built automatically and published to the GitHub Container Registry by the publish_image.yml workflow.\nDeployment can be summarized as follows:",
    "crumbs": [
      "Deployment"
    ]
  },
  {
    "objectID": "deployment.html#when-should-i-deploy-the-scrapers",
    "href": "deployment.html#when-should-i-deploy-the-scrapers",
    "title": "Deployment",
    "section": "When should I deploy the scrapers?",
    "text": "When should I deploy the scrapers?\nA main tag will be built automatically when a pull request is merged.\n\n\n\n\n\n\nCaution\n\n\n\nIf you have made a significant change to a scraper, it is recommended that you run that scrape on the staging Airflow instance before deploying the change to the production Airflow instance.\n\n\nA deploy tag will be built on pushes to the deploy branch. Once you are ready to deploy your change to production, run:\ngit push origin main:deploy\n\nA note on dependencies\nWe install our pupa and legistar dependencies from the master branch of their respective repositories, i.e., pip will not automatically recognize changes to these libraries.\n\n\n\n\n\n\nWarning\n\n\n\nIf you have made a change to pupa or legistar, you must rebuild the scraper images to deploy it.\n\n\nTo rebuild the scraper images without making a change to the scraper code, check out the main branch, then run:\ngit commit --allow-empty -m \"Rebuild scrapers\"\ngit push origin main && git push origin main:deploy",
    "crumbs": [
      "Deployment"
    ]
  },
  {
    "objectID": "scrapers/bills.html",
    "href": "scrapers/bills.html",
    "title": "Bill Scraper",
    "section": "",
    "text": "Located in lametro/bills.py\nCreates board reports and their associated attachments, sponsorships, votes, relations, and topics\nFurther reading: https://open-civic-data.readthedocs.io/en/latest/proposals/0006.html",
    "crumbs": [
      "Scrapers",
      "Bill Scraper"
    ]
  },
  {
    "objectID": "scrapers/bills.html#key-issues",
    "href": "scrapers/bills.html#key-issues",
    "title": "Bill Scraper",
    "section": "Key issues",
    "text": "Key issues\n\nBoard reports are added to the Legistar API before they are published to InSite. We need to determine which are “private” in order to suppress them in Councilmatic.\n\nWhy do we need to scrape private bills?\nLimit private scrape\n\nOur windowed scrapes rely on timestamps from the Legistar API, however the timestamps are not always updated when changes are made.\n\nMetro scraper did not scrape EventAgendaStatusName\nMetro: Scrape matters updated within a window, as well as matters related to events updated within a window",
    "crumbs": [
      "Scrapers",
      "Bill Scraper"
    ]
  },
  {
    "objectID": "scrapers/jurisdiction.html",
    "href": "scrapers/jurisdiction.html",
    "title": "Jurisdiction Scraper",
    "section": "",
    "text": "Located in lametro/__init__.py\nCreates jurisdiction, organization, and posts\n\nSpecifically, creates organizations that are not coded as the board itself or a committee in the Legistar API, as bodies meeting these parameters are created in the people scraper\n\nCreates legislative sessions",
    "crumbs": [
      "Scrapers",
      "Jurisdiction Scraper"
    ]
  },
  {
    "objectID": "scrapers/jurisdiction.html#core-concepts",
    "href": "scrapers/jurisdiction.html#core-concepts",
    "title": "Jurisdiction Scraper",
    "section": "Core concepts",
    "text": "Core concepts\n\nDivision\n\nPolitical geography\n“Jurisdictions exist within a division, while Posts can represent a division”\nDivisions relevant to LA Metro:\n\nCity of Los Angeles\nLos Angeles County supervisorial districts\nStatutorially defined transit sectors\nCaltrans (Calfornia Department of Transportation) District\n\nFurther reading: https://open-civic-data.readthedocs.io/en/latest/proposals/0002.html\n\n\n\nJurisdiction\n\nLogical unit of governance\nExample: LA Metro\nFurther reading: https://open-civic-data.readthedocs.io/en/latest/proposals/0003.html\n\n\n\nPost\n\nPosition in organization\nPosts define the core positions within an organization, and can optionally be associated with a Division, i.e., the political geography they represent\nExamples:\n\nPost associated with a division: Appointee of the Mayor of the City of Los Angeles on the Board of Directors representing the City of Los Angeles\nPost not associated with a division: Chair of the Board of Directors\n\nFurther reading: https://open-civic-data.readthedocs.io/en/latest/proposals/0005.html\n\n\n\nMembership\n\n“A relationship between a Person and an Organization, possibly including a Post”\n\nCommittee Memberships are not associated with a Post because committees do not have a defined membership structure\n\nMemberships are created by the people scraper\nFurther reading: https://open-civic-data.readthedocs.io/en/latest/proposals/0005.html",
    "crumbs": [
      "Scrapers",
      "Jurisdiction Scraper"
    ]
  },
  {
    "objectID": "scrapers/jurisdiction.html#key-issues",
    "href": "scrapers/jurisdiction.html#key-issues",
    "title": "Jurisdiction Scraper",
    "section": "Key issues",
    "text": "Key issues\n\nLegislative sessions must be added manually every June. The scraper breaks if they are not added before legislation is created in the new session.\n\nWrite a self-sufficient method for detecting LA Metro legislative session\nUpdate legislative session automatically",
    "crumbs": [
      "Scrapers",
      "Jurisdiction Scraper"
    ]
  }
]