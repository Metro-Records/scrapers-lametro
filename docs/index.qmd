---
title: "LA Metro Scrapers Documentation"
---

Welcome to the documentation for the LA Metro Scrapers! Here, you'll find
information about local development, deployment, and an overview of each
scraper (and decisions that we've made about them).

## How do they work?

At a high level, the scrapers retrieve information from Metro instances of
[the Legistar interface, also known as InSite](https://metro.legistar.com/Legislation.aspx),
and [the Legistar API](https://webapi.legistar.com/) (endpoints at `https://webapi.legistar.com/metro/*`).

See the relevant scraper documentation for more information about where information
comes from, and how it is parsed.

## How are they run?

The scrapers are run by Airflow and populate LA Metro Councilmatic instances,
outlined below.

| Scraper image tag | Airflow instance | Metro instance |
| - | - | - |
| `main` | [https://la-metro-dashboard-heroku.datamade.us/home](https://la-metro-dashboard-heroku.datamade.us/home) | [https://la-metro-councilmatic-staging.herokuapp.com/](https://la-metro-councilmatic-staging.herokuapp.com/) |
| `deploy` | [https://la-metro-dashboard-heroku-prod.datamade.us/home](https://la-metro-dashboard-heroku-prod.datamade.us/home) | [https://boardagendas.metro.net](https://boardagendas.metro.net) |

See [Deployment](deployment.qmd) for more on how scraper image tags are built.

## When do they run?

::: {.callout-tip}
See [the Airflow dashboard](https://la-metro-dashboard-heroku-prod.datamade.us/dashboard/) for information
about the latest and next scraper runs.
:::

```{python}
#| echo: false
import datetime
import pytz

def get_offset(tz):
  return abs(int(tz.utcoffset(datetime.datetime.now()).total_seconds() / (60*60)))

ordtz = pytz.timezone("America/Chicago")
latz = pytz.timezone("America/Los_Angeles")
```

::: {.callout-tip}
**Scrape schedules are written in UTC!** 

- Subtract `{python} get_offset(latz)` hours to convert to Los Angeles time.
- Subtract `{python} get_offset(ordtz)` hours to convert to Chicago time.

Mental math getting you down? [Try World Time Buddy](https://www.worldtimebuddy.com/?pl=1&lid=100,5368361,4887398&h=100&hf=1)!
:::

```{python}
#| echo: false
#| output: asis
from datetime import timedelta
import json

from cron_descriptor import get_description
import requests

config_file = requests.get("https://raw.githubusercontent.com/Metro-Records/la-metro-dashboard/refs/heads/main/dags/config.py")

config_start = False
_dag_config = []

for line in config_file.iter_lines():
    line_str = line.decode("utf-8")

    if line_str.startswith("SCRAPING_DAGS"):
        config_start = True
        continue

    if config_start:
        _dag_config.append(line_str)

exec("dag_config = {" + "".join(_dag_config))

for dag, conf in dag_config.items():
    print(f"### {dag}")

    print(f"_{conf['description']}_\n\n")
    for interval in conf["schedule_interval"]:
        print(f"- {get_description(interval)}\n")

    print("\n")
```

## What do they depend on?

The scrapers have a couple of key dependencies.

- [`pupa`](https://github.com/opencivicdata/pupa) is the framework for scraping and
organizing data according to the Open Civic Data standard. Our scrapers are subclasses of
`pupa.Scraper`, and we use the `pupa` CLI to run scrapes.
  - See [Useful pupa commands](http://localhost:3821/local-development.html#useful-pupa-commands)
  for more on the CLI.
- [`python-legistar-scraper`](https://github.com/opencivicdata/python-legistar-scraper/) is
a Python wrapper for InSite and the Legistar API that we use to retrieve data. Our scrapers
are also subclasses of the relevant `LegistarScraper` subclasses from this library.